{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1KWp0l0wPVVdyOl1ffhc8SY6TOKkUXjxw",
      "authorship_tag": "ABX9TyPPtxt0rBe0ZkOy9GrIgN+1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stefi96/DetectingNFTs-Master/blob/main/Images_NFTs_Master.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OR4v2ayYfrNW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to all the scam and legit images\n",
        "# NOTE: One option is the paths for google cloud and the other for running locally\n",
        "scam_image_paths = [os.path.join(\"/content/drive/MyDrive/Master/Project/Images/Images_scam\", image) for image in os.listdir(\"/content/drive/MyDrive/Master/Project/Images/Images_scam\")]\n",
        "legit_image_paths = [os.path.join(\"/content/drive/MyDrive/Master/Project/Images/Images_legit\", image) for image in os.listdir(\"/content/drive/MyDrive/Master/Project/Images/Images_legit\")]\n",
        "#scam_image_paths = [os.path.join(r\"C:\\Users\\stefanve\\Desktop\\Project\\Images\\Images_scam\", image) for image in os.listdir(r\"C:\\Users\\stefanve\\Desktop\\Project\\Images\\Images_scam\")]\n",
        "#legit_image_paths = [os.path.join(r\"C:\\Users\\stefanve\\Desktop\\Project\\Images\\Images_legit\", image) for image in os.listdir(r\"C:\\Users\\stefanve\\Desktop\\Project\\Images\\Images_legit\")]"
      ],
      "metadata": {
        "id": "YW4WzuAlhR9l"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to your scam and legit images on Google Drive\n",
        "scam_dir = \"/content/drive/MyDrive/Master/Project/Images/Images_scam\"\n",
        "legit_dir = \"/content/drive/MyDrive/Master/Project/Images/Images_legit\"\n",
        "\n",
        "# List the images in the directories\n",
        "scam_images = os.listdir(scam_dir)\n",
        "legit_images = os.listdir(legit_dir)\n",
        "\n",
        "# Print the number of images detected in each directory\n",
        "print(f\"Number of scam images detected: {len(scam_images)}\")\n",
        "print(f\"Number of legit images detected: {len(legit_images)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cV51vGgixsO",
        "outputId": "9a136291-11fa-4809-e353-1188ffdbf60d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of scam images detected: 1082\n",
            "Number of legit images detected: 1762\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def try_loading_images(image_paths):\n",
        "    loaded_images = []\n",
        "    for img_path in image_paths[:10]:  # Limiting to the first 10 images for testing\n",
        "        try:\n",
        "            img = Image.open(img_path).resize((128, 128))\n",
        "            img_array = np.array(img) / 255.0\n",
        "            loaded_images.append(img_array)\n",
        "        except Exception as e:\n",
        "            print(f\"Error with image {img_path}: {e}\")\n",
        "    return loaded_images\n",
        "\n",
        "# Attempt to load and process a subset of scam and legit images\n",
        "loaded_scam_images = try_loading_images(scam_image_paths)\n",
        "loaded_legit_images = try_loading_images(legit_image_paths)\n",
        "\n",
        "print(f\"Number of scam images loaded: {len(loaded_scam_images)}\")\n",
        "print(f\"Number of legit images loaded: {len(loaded_legit_images)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyA-YvkQj4DT",
        "outputId": "7df6d94f-cf32-433e-a1af-e2dc46da19ab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of scam images loaded: 10\n",
            "Number of legit images loaded: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_images_modified(image_paths, label):\n",
        "    data = []\n",
        "    labels = []\n",
        "    for img_path in image_paths:\n",
        "        try:\n",
        "            # Open the image and convert to RGB (removing any alpha channel)\n",
        "            img = Image.open(img_path).convert(\"RGB\").resize((128, 128))\n",
        "            img_array = np.array(img) / 255.0\n",
        "            data.append(img_array)\n",
        "            labels.append(label)\n",
        "        except Exception as e:\n",
        "            print(f\"Error with image {img_path}: {e}\")\n",
        "    return data, labels\n",
        "\n",
        "# Load and preprocess the images using the modified function\n",
        "scam_data, scam_labels = load_and_preprocess_images_modified(scam_image_paths, \"scam\")\n",
        "legit_data, legit_labels = load_and_preprocess_images_modified(legit_image_paths, \"legit\")\n",
        "\n",
        "# Combine data and labels\n",
        "all_data = scam_data + legit_data\n",
        "all_labels = scam_labels + legit_labels\n",
        "\n",
        "print(f\"Total number of images in all_data: {len(all_data)}\")\n",
        "print(f\"Total number of labels in all_labels: {len(all_labels)}\")"
      ],
      "metadata": {
        "id": "g4UDM4grniEQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bfd850a-384c-4035-c9a5-bf3994223685"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of images in all_data: 2844\n",
            "Total number of labels in all_labels: 2844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the debug function to load and preprocess images\n",
        "scam_data, scam_labels = load_and_preprocess_images_modified(scam_image_paths, \"scam\")\n",
        "legit_data, legit_labels = load_and_preprocess_images_modified(legit_image_paths, \"legit\")"
      ],
      "metadata": {
        "id": "NZfMjHIcqUqC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = scam_data + legit_data\n",
        "all_labels = scam_labels + legit_labels\n",
        "\n",
        "print(f\"Total number of images in all_data: {len(all_data)}\")\n",
        "print(f\"Total number of labels in all_labels: {len(all_labels)}\")"
      ],
      "metadata": {
        "id": "NBIev345qV4V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b60409f8-a224-462d-87ac-6985b4f10471"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of images in all_data: 2844\n",
            "Total number of labels in all_labels: 2844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(all_data, all_labels, test_size=0.3, random_state=42, stratify=all_labels)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "print(f\"Training data size: {len(X_train)}\")\n",
        "print(f\"Validation data size: {len(X_val)}\")\n",
        "print(f\"Test data size: {len(X_test)}\")"
      ],
      "metadata": {
        "id": "DfPlTm9VqV1_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "320dd4cf-aa59-42d4-b583-80246a73905f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data size: 1990\n",
            "Validation data size: 427\n",
            "Test data size: 427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "41VaIuEjNaU6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(y_train)\n",
        "y_val = label_encoder.transform(y_val)\n",
        "y_test = label_encoder.transform(y_test)\n"
      ],
      "metadata": {
        "id": "FO8HN5qThgV8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(y_train, 2)\n",
        "y_val = to_categorical(y_val, 2)\n",
        "y_test = to_categorical(y_test, 2)"
      ],
      "metadata": {
        "id": "RLSpLaqphgPW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (128, 128, 3)\n",
        "\n",
        "# Adding dropout layers\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "model.add(MaxPooling2D(2, 2))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(2, 2))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation='softmax'))"
      ],
      "metadata": {
        "id": "IGaRMoK8hgNE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "AeL1gRDpznpy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding early stopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "aLuD6UmPzo7P"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "datagen.fit(X_train)"
      ],
      "metadata": {
        "id": "TMT7q6FlzrjT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "X_val = np.array(X_val)\n",
        "y_val = np.array(y_val)"
      ],
      "metadata": {
        "id": "cn6XJ2c10ViC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_val shape:\", X_val.shape)\n",
        "print(\"y_val shape:\", y_val.shape)"
      ],
      "metadata": {
        "id": "cDxQhK2n0Wl2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "707d36e0-7c5f-4d44-ddf9-bb8f8ebdd130"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (1990, 128, 128, 3)\n",
            "y_train shape: (1990, 2)\n",
            "X_val shape: (427, 128, 128, 3)\n",
            "y_val shape: (427, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_epochs = 50  # Adjust this based on your requirements\n",
        "history = model.fit(datagen.flow(np.array(X_train), np.array(y_train), batch_size=32),\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    epochs=number_of_epochs,\n",
        "                    callbacks=[early_stop])\n",
        "#history = model.fit(np.array(X_train), np.array(y_train), epochs=10, batch_size=32, validation_data=(np.array(X_val), np.array(y_val)), callbacks=[early_stop])"
      ],
      "metadata": {
        "id": "vtNDA-LThgKP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "outputId": "bd680d44-67c5-427d-c603-7ec62f74a0b3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "63/63 [==============================] - 81s 1s/step - loss: 0.8426 - accuracy: 0.7990 - val_loss: 0.3151 - val_accuracy: 0.8899\n",
            "Epoch 2/50\n",
            "63/63 [==============================] - 79s 1s/step - loss: 0.2282 - accuracy: 0.9261 - val_loss: 0.2435 - val_accuracy: 0.9508\n",
            "Epoch 3/50\n",
            "63/63 [==============================] - 79s 1s/step - loss: 0.1860 - accuracy: 0.9497 - val_loss: 0.1832 - val_accuracy: 0.9321\n",
            "Epoch 4/50\n",
            "63/63 [==============================] - 106s 2s/step - loss: 0.1748 - accuracy: 0.9538 - val_loss: 0.2127 - val_accuracy: 0.9391\n",
            "Epoch 5/50\n",
            "63/63 [==============================] - 85s 1s/step - loss: 0.1353 - accuracy: 0.9613 - val_loss: 0.1429 - val_accuracy: 0.9625\n",
            "Epoch 6/50\n",
            "63/63 [==============================] - 83s 1s/step - loss: 0.1203 - accuracy: 0.9628 - val_loss: 0.1391 - val_accuracy: 0.9672\n",
            "Epoch 7/50\n",
            "63/63 [==============================] - 85s 1s/step - loss: 0.1081 - accuracy: 0.9663 - val_loss: 0.1306 - val_accuracy: 0.9602\n",
            "Epoch 8/50\n",
            "63/63 [==============================] - 82s 1s/step - loss: 0.1106 - accuracy: 0.9663 - val_loss: 0.1395 - val_accuracy: 0.9649\n",
            "Epoch 9/50\n",
            "63/63 [==============================] - 85s 1s/step - loss: 0.1093 - accuracy: 0.9673 - val_loss: 0.1229 - val_accuracy: 0.9696\n",
            "Epoch 10/50\n",
            "63/63 [==============================] - 85s 1s/step - loss: 0.1077 - accuracy: 0.9668 - val_loss: 0.1142 - val_accuracy: 0.9696\n",
            "Epoch 11/50\n",
            "63/63 [==============================] - 81s 1s/step - loss: 0.1108 - accuracy: 0.9633 - val_loss: 0.1174 - val_accuracy: 0.9696\n",
            "Epoch 12/50\n",
            "63/63 [==============================] - 83s 1s/step - loss: 0.0891 - accuracy: 0.9729 - val_loss: 0.1168 - val_accuracy: 0.9742\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-66be37b4869b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnumber_of_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m  \u001b[0;31m# Adjust this based on your requirements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = model.fit(datagen.flow(np.array(X_train), np.array(y_train), batch_size=32),\n\u001b[0m\u001b[1;32m      3\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumber_of_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     callbacks=[early_stop])\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1671\u001b[0m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1673\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1674\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2426\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2428\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2429\u001b[0m         \"\"\"Resets the state of all the metrics in the model.\n\u001b[1;32m   2430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(np.array(X_test), np.array(y_test))\n",
        "print(f\"Test accuracy: {test_acc}\")"
      ],
      "metadata": {
        "id": "GEYIFTrNhoWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Original X_test shape: {np.array(X_test).shape}\")\n",
        "\n",
        "# Reshape if necessary\n",
        "X_test_reshaped = np.array(X_test).reshape(-1, 128, 128, 3)  # Assuming your images are 128x128 and RGB\n",
        "print(f\"Reshaped X_test shape: {X_test_reshaped.shape}\")"
      ],
      "metadata": {
        "id": "Ur2tnjR1hoSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "\n",
        "# Predict the labels for the test set using reshaped data\n",
        "y_pred = model.predict(X_test_reshaped)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred_classes))\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_true, y_pred_classes))\n",
        "\n",
        "# ROC-AUC\n",
        "# Note: This is for binary classification. If your task is multi-class, this will need adjustments.\n",
        "roc_auc = roc_auc_score(y_true, y_pred_classes)\n",
        "print(f\"\\nROC-AUC: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "Q3XCJC4bt2O_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting training & validation accuracy\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training vs Validation Accuracy')\n",
        "\n",
        "# Plotting training & validation loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FNNEiTNot3Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transfer learning\n",
        "from keras.applications import ResNet50\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "r7KduMrSt4aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "datagen.fit(X_train)"
      ],
      "metadata": {
        "id": "DVhZNtXqkWaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(128, 128, 3))"
      ],
      "metadata": {
        "id": "3VrokqKexNk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)  # Global pooling layer\n",
        "x = Dropout(0.5)(x)  # Add dropout for regularization\n",
        "predictions = Dense(2, activation='softmax')(x)  # Final classification layer\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)"
      ],
      "metadata": {
        "id": "xVw8MR3Tkhz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "gHrgZhn2Ox2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "rWaCR3M8klHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# only for collab\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "X_val = np.array(X_val)\n",
        "y_val = np.array(y_val)"
      ],
      "metadata": {
        "id": "_VdmDNmwJNPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # only for collab\n",
        "# from keras.utils import to_categorical\n",
        "\n",
        "# # Map the string labels to integers\n",
        "# label_mapping = {\"scam\": 0, \"legit\": 1}\n",
        "\n",
        "# y_train_int = np.array([label_mapping[label] for label in y_train])\n",
        "# y_val_int = np.array([label_mapping[label] for label in y_val])\n",
        "\n",
        "# # Now apply one-hot encoding\n",
        "# y_train = to_categorical(y_train_int, num_classes=2)\n",
        "# y_val = to_categorical(y_val_int, num_classes=2)"
      ],
      "metadata": {
        "id": "yHETIye9JXEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# only for collab\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n"
      ],
      "metadata": {
        "id": "oCXu1pVoJYE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define early stopping, if necesarry patience can be 15\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
        "\n",
        "# Train using data augmentation\n",
        "history = model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    epochs=10)"
      ],
      "metadata": {
        "id": "jvaz9s60xVTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning: Unfreeze the last 4 layers of the base model\n",
        "for layer in base_model.layers[-4:]:\n",
        "    layer.trainable = True"
      ],
      "metadata": {
        "id": "TUZZIMZltY4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Continue training with a lower learning rate\n",
        "history_fine = model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
        "                         validation_data=(X_val, y_val),\n",
        "                         epochs=50)"
      ],
      "metadata": {
        "id": "C6OlOjBytY2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(np.array(X_test), np.array(y_test))\n",
        "print(f\"Test accuracy: {test_acc}\")"
      ],
      "metadata": {
        "id": "e0fQ8o_UxX68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "\n",
        "# Predict the labels for the test set using reshaped data\n",
        "X_test = np.array(X_test)\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred_classes))\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_true, y_pred_classes))\n",
        "\n",
        "# ROC-AUC\n",
        "# Note: This is for binary classification. If your task is multi-class, this will need adjustments.\n",
        "roc_auc = roc_auc_score(y_true, y_pred_classes)\n",
        "print(f\"\\nROC-AUC: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "J8-Ou5zJxa9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Plotting training & validation accuracy\n",
        "# plt.figure(figsize=(12, 4))\n",
        "# plt.subplot(1, 2, 1)\n",
        "# plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "# plt.legend()\n",
        "# plt.title('Training vs Validation Accuracy')\n",
        "\n",
        "# # Plotting training & validation loss\n",
        "# plt.subplot(1, 2, 2)\n",
        "# plt.plot(history.history['loss'], label='Training Loss')\n",
        "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "# plt.legend()\n",
        "# plt.title('Training vs Validation Loss')\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'] + history_fine.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'] + history_fine.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training vs Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'] + history_fine.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'] + history_fine.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EtY-8VrwxhlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from keras.layers import Bidirectional\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "# Assuming X_train and X_val are your training and validation data\n",
        "# Reshape the data to match the expected input shape\n",
        "X_train_reshaped = X_train.reshape(X_train.shape[0], 128, 128 * 3)\n",
        "X_val_reshaped = X_val.reshape(X_val.shape[0], 128, 128 * 3)\n",
        "\n",
        "# Define the RNN model\n",
        "model = Sequential()\n",
        "\n",
        "# Add Bidirectional LSTM layers with increased units\n",
        "model.add(Bidirectional(LSTM(100, return_sequences=True, activation='tanh'), input_shape=(128, 128 * 3)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Bidirectional(LSTM(100, activation='tanh')))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Classification layer\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Define early stopping and learning rate reduction on plateau\n",
        "from keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_reshaped, y_train,\n",
        "    validation_data=(X_val_reshaped, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")"
      ],
      "metadata": {
        "id": "DBwZYizJxjdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.array(X_test)\n",
        "X_test_reshaped = X_test.reshape(X_test.shape[0], 128, 128 * 3)\n",
        "y_test = np.array(y_test)\n",
        "print(\"Shape of X_test_reshaped:\", X_test_reshaped.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "metadata": {
        "id": "nMPiHr7o-zp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(X_test_reshaped, y_test)\n",
        "print(f\"Test accuracy: {test_acc}\")"
      ],
      "metadata": {
        "id": "WRlLD-TZ-UIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the labels for the test set using reshaped data\n",
        "y_pred = model.predict(X_test_reshaped)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred_classes))\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_true, y_pred_classes))\n",
        "\n",
        "# ROC-AUC\n",
        "# Note: This is for binary classification. If your task is multi-class, this will need adjustments.\n",
        "roc_auc = roc_auc_score(y_true, y_pred_classes)\n",
        "print(f\"\\nROC-AUC: {roc_auc:.4f}\")"
      ],
      "metadata": {
        "id": "aaeY5HYW9Uu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting training & validation accuracy\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training vs Validation Accuracy')\n",
        "\n",
        "# Plotting training & validation loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7cMaBUtL-W4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications import VGG16\n",
        "from keras.models import Model\n",
        "from keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Load VGG16 without top classification layer\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "\n",
        "# Add custom layers\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(2, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze VGG16 layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Data Augmentation (optional but recommended for small datasets)\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Train the model using augmented data\n",
        "history = model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    epochs=10)\n"
      ],
      "metadata": {
        "id": "wxfwouBaWk6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_test = np.array(X_test)\n",
        "# X_test_reshaped = X_test.reshape(X_test.shape[0], 128, 128 * 3)\n",
        "# y_test = np.array(y_test)\n",
        "# print(\"Shape of X_test_reshaped:\", X_test_reshaped.shape)\n",
        "# print(\"Shape of y_test:\", y_test.shape)"
      ],
      "metadata": {
        "id": "BLNR61ChH_p2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = preprocess_input(X_test)"
      ],
      "metadata": {
        "id": "2nemse5wKC9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy: {test_acc}\")"
      ],
      "metadata": {
        "id": "iIQqtncQsVQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert one-hot encoded labels back to label format for visualization\n",
        "y_train_labels = np.argmax(y_train, axis=1)\n",
        "\n",
        "# Count class distribution\n",
        "(unique, counts) = np.unique(y_train_labels, return_counts=True)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(unique, counts, tick_label=[\"Scam\", \"Legit\"])  # Assuming 0: Scam and 1: Legit\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Number of samples')\n",
        "plt.title('Class Distribution in Training Set')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VuTfS4GyH7i1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define a function to display sample images from each class\n",
        "def display_sample_images(X, y, label_mapping, num_samples=5):\n",
        "    fig, axes = plt.subplots(2, num_samples, figsize=(15, 6))\n",
        "\n",
        "    for label, label_str in label_mapping.items():\n",
        "        # Get indices of images belonging to the current label\n",
        "        indices = np.where(y == label)[0]\n",
        "\n",
        "        # Randomly select 'num_samples' indices\n",
        "        selected_indices = np.random.choice(indices, num_samples, replace=False)\n",
        "\n",
        "        for idx, ax in enumerate(axes[label]):\n",
        "            ax.imshow(X[selected_indices[idx]])\n",
        "            ax.set_title(label_str)\n",
        "            ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Convert one-hot encoded labels back to integer labels for easier indexing\n",
        "y_train_int = np.argmax(y_train, axis=1)\n",
        "\n",
        "# Define the label mapping based on the y_train data\n",
        "label_mapping = {0: 'Legit', 1: 'Scam'}\n",
        "\n",
        "# Display sample images\n",
        "display_sample_images(X_train, y_train_int, label_mapping)"
      ],
      "metadata": {
        "id": "8HqsgpIDQvXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from keras.applications.vgg16 import VGG16\n",
        "# from keras.layers import LSTM, Dense, Dropout, TimeDistributed\n",
        "# from keras.models import Sequential\n",
        "\n",
        "# # 1. Load the VGG16 model without the top classification layer\n",
        "# base_model = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "\n",
        "# # Make sure VGG16 layers are not trainable\n",
        "# for layer in base_model.layers:\n",
        "#     layer.trainable = False\n",
        "\n",
        "# # 2. Define the sequential model\n",
        "# model = Sequential()\n",
        "\n",
        "# # Add VGG16 as the feature extractor\n",
        "# model.add(base_model)\n",
        "\n",
        "# # Convert the features to sequences for LSTM\n",
        "# model.add(TimeDistributed(Flatten()))\n",
        "\n",
        "# # 3. Add LSTM layer\n",
        "# model.add(LSTM(100, activation='tanh'))\n",
        "\n",
        "# # Some dropout for regularization\n",
        "# model.add(Dropout(0.5))\n",
        "\n",
        "# # Classification layer\n",
        "# model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# # Train the model\n",
        "# # Assuming X_train_reshaped, y_train, X_val_reshaped, and y_val are already prepared\n",
        "# history = model.fit(X_train_reshaped, y_train, validation_data=(X_val_reshaped, y_val), epochs=10, batch_size=32)\n"
      ],
      "metadata": {
        "id": "jsDYgOsyZZuj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}